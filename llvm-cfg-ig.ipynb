{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1f052890",
   "metadata": {},
   "source": [
    "# LLVM-IR â†’ CFG â†’ Grammar Extraction\n",
    "This notebook contains the core logic from the project split into individual, executable cells. Each class and standalone function is isolated in its own cell to facilitate exploration and interactive experimentation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f53f394",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from typing import Dict, List, Set, Tuple, Optional\n",
    "from collections import defaultdict, deque\n",
    "from dataclasses import dataclass\n",
    "import os\n",
    "from typing import Dict, List, Tuple, Optional\n",
    "from llvm_cfg_generator import llvm_ir_to_context_free_grammar, ContextFreeGrammar\n",
    "from typing import Dict, List, Optional"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "919cb054",
   "metadata": {},
   "source": [
    "### `BasicBlock` *(from `llvm_cfg_generator.py`)*\n",
    "\n",
    "Immutable metadata holder that models an LLVM *basic block* inside a\n",
    "function-level control-flow graph (CFG).\n",
    "\n",
    "In the textual LLVM IR a basic block is introduced by a label such as\n",
    "\n",
    "    ``entry:``\n",
    "\n",
    "followed by a *contiguous* list of instructions that concludes with a\n",
    "single *terminator* (`br`, `switch`, `ret`, `invoke`, â€¦).  The absence of\n",
    "explicit `goto`-style jump statements within the body guarantees that\n",
    "**all** control-flow into the block is funnelled through the label and\n",
    "**all** control-flow out of the block emanates from the last instruction.\n",
    "\n",
    "The present class does not attempt any semantic inspection of the\n",
    "instructions; it simply acts as a lightweight container so that the CFG\n",
    "construction algorithm can record structural relationships.\n",
    "\n",
    "Attributes\n",
    "----------\n",
    "name:\n",
    "    Canonical label that uniquely identifies the block inside its\n",
    "    enclosing function.\n",
    "instructions:\n",
    "    List of raw LLVM textual lines *exactly* as they appear in the input\n",
    "    file.  The list is filled incrementally while the parser iterates over\n",
    "    the function body.\n",
    "successors / predecessors:\n",
    "    Outgoing and incoming adjacency lists respectively.  They are updated\n",
    "    by :pyclass:`ControlFlowGraph.add_edge` once the parser encounters a\n",
    "    terminator that creates a control-flow edge."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd24a0b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BasicBlock:\n",
    "    name: str\n",
    "    instructions: List[str]\n",
    "    successors: List[str]\n",
    "    predecessors: List[str]\n",
    "    \n",
    "    def __init__(self, name: str):\n",
    "        self.name = name\n",
    "        self.instructions = []\n",
    "        self.successors = []\n",
    "        self.predecessors = []"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "100ab32a",
   "metadata": {},
   "source": [
    "### `CFGEdge` *(from `llvm_cfg_generator.py`)*\n",
    "\n",
    "Directed *hyper-edge* (source â†’ target) that connects two\n",
    ":pyclass:`BasicBlock` instances in a :pyclass:`ControlFlowGraph`.\n",
    "\n",
    "The optional *condition* label provides minimal semantic context that is\n",
    "later used by the grammar generator to create *choice* non-terminals.\n",
    "Examples include ``\"true\"`` / ``\"false\"`` for a conditional `br`,\n",
    "``\"switch\"`` for a `switch`-based dispatch, or ``None`` for an\n",
    "unconditional jump."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93240640",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CFGEdge:\n",
    "    source: str\n",
    "    target: str\n",
    "    condition: Optional[str] = None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8c02244",
   "metadata": {},
   "source": [
    "### `ControlFlowGraph` *(from `llvm_cfg_generator.py`)*\n",
    "\n",
    "Adjacency-list representation of the intra-procedural control-flow graph\n",
    "that underpins the entire *IR â†’ Grammar* pipeline.\n",
    "\n",
    "Design choices\n",
    "--------------\n",
    "â€¢ **No edge de-duplication:** Multiple syntactically different IR\n",
    "  constructs can yield identical source/target pairs â€“ e.g. two\n",
    "  back-to-back `br` instructions protected by mutually exclusive\n",
    "  predicates.  Retaining duplicates helps the grammar generator produce a\n",
    "  richer set of rules that account for *how* control reaches a block.\n",
    "\n",
    "â€¢ **Partial connectivity:** The parser deliberately allows *dangling*\n",
    "  blocks that have no successors (e.g. `unreachable`) or no predecessors\n",
    "  (dead code after DCE). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b26bb7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ControlFlowGraph:\n",
    "    def __init__(self):\n",
    "        self.blocks: Dict[str, BasicBlock] = {}\n",
    "        self.edges: List[CFGEdge] = []\n",
    "        self.entry_block: Optional[str] = None\n",
    "        self.exit_blocks: Set[str] = set()\n",
    "    \n",
    "    def add_block(self, name: str) -> BasicBlock:\n",
    "        if name not in self.blocks:\n",
    "            self.blocks[name] = BasicBlock(name)\n",
    "        return self.blocks[name]\n",
    "    \n",
    "    def add_edge(self, source: str, target: str, condition: Optional[str] = None):\n",
    "        edge = CFGEdge(source, target, condition)\n",
    "        self.edges.append(edge)\n",
    "        \n",
    "        if source in self.blocks:\n",
    "            self.blocks[source].successors.append(target)\n",
    "        if target in self.blocks:\n",
    "            self.blocks[target].predecessors.append(source)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b4f3933",
   "metadata": {},
   "source": [
    "### `GrammarRule` *(from `llvm_cfg_generator.py`)*\n",
    "\n",
    "Single production in a context-free grammar following the conventional\n",
    "Backusâ€“Naur notation\n",
    "\n",
    "    ``<lhs> ::= rhsâ‚ rhsâ‚‚ â€¦ rhsâ‚™``\n",
    "\n",
    "where *rhsáµ¢* may itself be of terminal or non-terminal category.  The\n",
    "class is intentionally *dumb* â€“ it stores raw strings only â€“ because the\n",
    "surrounding framework maintains global sets of terminals/non-terminals and\n",
    "invariants such as *reachability* and *productivity*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6dd6c500",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GrammarRule:\n",
    "    def __init__(self, lhs: str, rhs: List[str]):\n",
    "        self.lhs = lhs  # Left-hand side (non-terminal)\n",
    "        self.rhs = rhs  # Right-hand side (list of terminals/non-terminals)\n",
    "    \n",
    "    def __str__(self):\n",
    "        return f\"{self.lhs} -> {' '.join(self.rhs)}\"\n",
    "    \n",
    "    def __repr__(self):\n",
    "        return self.__str__()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67694d67",
   "metadata": {},
   "source": [
    "### `ContextFreeGrammar` *(from `llvm_cfg_generator.py`)*\n",
    "\n",
    "Aggregates the production rules that collectively form the *executable\n",
    "model* of an LLVM function.\n",
    "\n",
    "Apart from storing the rules themselves, the class keeps canonicalised\n",
    "*sets* of terminals and non-terminals which are updated incrementally as\n",
    "new rules are added.  This affords O(1) membership tests when the grammar\n",
    "builder checks whether a symbol has been introduced before."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab842b36",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ContextFreeGrammar:\n",
    "    def __init__(self):\n",
    "        self.rules: List[GrammarRule] = []\n",
    "        self.terminals: Set[str] = set()\n",
    "        self.non_terminals: Set[str] = set()\n",
    "        self.start_symbol: str = \"S\"\n",
    "    \n",
    "    def add_rule(self, lhs: str, rhs: List[str]):\n",
    "        rule = GrammarRule(lhs, rhs)\n",
    "        self.rules.append(rule)\n",
    "        self.non_terminals.add(lhs)\n",
    "        \n",
    "        for symbol in rhs:\n",
    "            if self.is_terminal(symbol):\n",
    "                self.terminals.add(symbol)\n",
    "            else:\n",
    "                self.non_terminals.add(symbol)\n",
    "    \n",
    "    def is_terminal(self, symbol: str) -> bool:\n",
    "        \"\"\"Determine if a symbol is a terminal (enhanced pattern matching)\"\"\"\n",
    "        # Terminals are typically instruction names, constants, or specific tokens\n",
    "        terminal_patterns = [\n",
    "            r'^[A-Z][A-Z_]*$',  # All caps tokens like ADD, LOAD, STORE\n",
    "            r'^[a-z]+$',        # Simple instruction names like 'add', 'load', 'store'\n",
    "            r'^%\\w+$',          # LLVM registers\n",
    "            r'^@\\w+$',          # LLVM global symbols\n",
    "            r'^\\d+$',           # Constants\n",
    "            r'^\".*\"$',          # String literals\n",
    "            r'^EPSILON$',       # Empty string terminal\n",
    "            r'^IF$|^THEN$|^ELSE$|^WHILE$|^DO$|^ASSIGN$|^COMPARE$|^VARIABLE$|^CONSTANT$|^EXPRESSION$',  # Control flow keywords\n",
    "            r'^(ENTRY|EXIT|CONTINUE|BREAK|JUMP|FALLTHROUGH)$',  # Enhanced control flow\n",
    "            r'^(ALLOCA|LOAD|STORE|GEP|BITCAST|INTTOPTR|PTRTOINT)$',  # Memory operations\n",
    "            r'^(PHI|SELECT|EXTRACTVALUE|INSERTVALUE)$',  # Data flow operations\n",
    "            r'^(INVOKE|LANDINGPAD|RESUME|UNREACHABLE)$',  # Exception handling\n",
    "            r'^(FENCE|ATOMICRMW|CMPXCHG)$',  # Atomic operations\n",
    "            r'^LABEL_\\d+$',     # Basic block labels\n",
    "            r'^ARG_\\d+$',       # Function arguments\n",
    "            r'^(NULL|VOID|TRUE|FALSE)$'  # Constants\n",
    "        ]\n",
    "        \n",
    "        for pattern in terminal_patterns:\n",
    "            if re.match(pattern, symbol):\n",
    "                return True\n",
    "        return False\n",
    "    \n",
    "    def __str__(self):\n",
    "        result = []\n",
    "        result.append(f\"Start Symbol: {self.start_symbol}\")\n",
    "        result.append(f\"Non-terminals: {{{', '.join(sorted(self.non_terminals))}}}\")\n",
    "        result.append(f\"Terminals: {{{', '.join(sorted(self.terminals))}}}\")\n",
    "        result.append(\"Production Rules:\")\n",
    "        for rule in self.rules:\n",
    "            result.append(f\"  {rule}\")\n",
    "        return '\\n'.join(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00a556c6",
   "metadata": {},
   "source": [
    "### `LLVMIRParser` *(from `llvm_cfg_generator.py`)*\n",
    "\n",
    "Streaming LLVM-IR parser whose only responsibility is to recover *control\n",
    "structure* â€“ it consciously ignores type information, bit-widths, and\n",
    "other semantic details.\n",
    "\n",
    "Implementation highlights\n",
    "-------------------------\n",
    "â€¢ **Regex-centric:** Leveraging a handful of high-precision regular\n",
    "  expressions avoids the maintenance overhead of a fullâ€blown IR grammar.\n",
    "\n",
    "â€¢ **Resilience over completeness:** Whenever the parser encounters an\n",
    "  exotic construct it cannot handle, the error is *contained* to that\n",
    "  function; the surrounding module continues to be processed so that fuzz\n",
    "  campaigns are not blocked by a single unparseable corner case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "591624e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LLVMIRParser:\n",
    "    \n",
    "    def __init__(self):\n",
    "        # Enhanced patterns for modern LLVM IR\n",
    "        self.function_pattern = re.compile(r'define\\s+(?:dso_local\\s+|internal\\s+)?\\w+\\s+@(\\w+)\\s*\\([^)]*\\)\\s*(?:#\\d+\\s*)?\\{', re.IGNORECASE)\n",
    "        self.block_pattern = re.compile(r'^(\\w+):\\s*(?:;.*)?$')\n",
    "        self.numbered_block_pattern = re.compile(r'^(\\d+):\\s*(?:;.*)?$')\n",
    "        self.branch_pattern = re.compile(r'br\\s+i1\\s+[^,]+,\\s+label\\s+%(\\w+),\\s+label\\s+%(\\w+)')\n",
    "        self.unconditional_branch_pattern = re.compile(r'br\\s+label\\s+%(\\w+)')\n",
    "        self.return_pattern = re.compile(r'ret\\s+')\n",
    "        self.call_pattern = re.compile(r'call\\s+.*?@(\\w+)')\n",
    "        self.instruction_pattern = re.compile(r'^\\s*(?:%\\w+\\s*=\\s*)?(\\w+)\\s+(.*)')\n",
    "        self.switch_pattern = re.compile(r'switch\\s+.*?\\[([^\\]]+)\\]')\n",
    "        self.indirectbr_pattern = re.compile(r'indirectbr\\s+.*?\\[([^\\]]+)\\]')\n",
    "    \n",
    "    def parse_llvm_ir(self, llvm_code: str) -> Dict[str, ControlFlowGraph]:\n",
    "        \"\"\"Parse LLVM-IR code and extract CFGs for each function with enhanced error handling\"\"\"\n",
    "        functions = {}\n",
    "        \n",
    "        try:\n",
    "            # Split into functions with better handling\n",
    "            function_blocks = self._split_into_functions_enhanced(llvm_code)\n",
    "            \n",
    "            for func_name, func_code in function_blocks.items():\n",
    "                try:\n",
    "                    cfg = self._build_cfg_from_function(func_name, func_code)\n",
    "                    if cfg.blocks:  # Only add non-empty CFGs\n",
    "                        functions[func_name] = cfg\n",
    "                except Exception as e:\n",
    "                    print(f\"Warning: Failed to parse function {func_name}: {e}\")\n",
    "                    continue\n",
    "        except Exception as e:\n",
    "            print(f\"Error parsing LLVM IR: {e}\")\n",
    "        \n",
    "        return functions\n",
    "    \n",
    "    def _split_into_functions_enhanced(self, llvm_code: str) -> Dict[str, str]:\n",
    "        \"\"\"Enhanced function splitting with better pattern matching\"\"\"\n",
    "        functions = {}\n",
    "        lines = llvm_code.split('\\n')\n",
    "        current_function = None\n",
    "        current_code = []\n",
    "        brace_count = 0\n",
    "        in_function = False\n",
    "        \n",
    "        for i, line in enumerate(lines):\n",
    "            original_line = line\n",
    "            line = line.strip()\n",
    "            \n",
    "            # Skip empty lines and global declarations outside functions\n",
    "            if not line or (line.startswith(';') and not in_function):\n",
    "                continue\n",
    "            \n",
    "            # Check for function definition with enhanced pattern\n",
    "            func_match = self.function_pattern.search(line)\n",
    "            if func_match:\n",
    "                # Save previous function if exists\n",
    "                if current_function and current_code:\n",
    "                    functions[current_function] = '\\n'.join(current_code)\n",
    "                \n",
    "                current_function = func_match.group(1)\n",
    "                current_code = [original_line]\n",
    "                brace_count = line.count('{') - line.count('}')\n",
    "                in_function = True\n",
    "                continue\n",
    "            \n",
    "            if in_function and current_function:\n",
    "                current_code.append(original_line)\n",
    "                brace_count += line.count('{') - line.count('}')\n",
    "                \n",
    "                # Function ends when braces are balanced\n",
    "                if brace_count == 0:\n",
    "                    functions[current_function] = '\\n'.join(current_code)\n",
    "                    current_function = None\n",
    "                    current_code = []\n",
    "                    in_function = False\n",
    "        \n",
    "        # Handle last function if file doesn't end with closing brace\n",
    "        if current_function and current_code:\n",
    "            functions[current_function] = '\\n'.join(current_code)\n",
    "        \n",
    "        return functions\n",
    "    \n",
    "    def _build_cfg_from_function(self, func_name: str, func_code: str) -> ControlFlowGraph:\n",
    "        \"\"\"Build CFG from a single function's LLVM-IR code with enhanced block detection\"\"\"\n",
    "        cfg = ControlFlowGraph()\n",
    "        lines = func_code.split('\\n')\n",
    "        \n",
    "        current_block = None\n",
    "        entry_found = False\n",
    "        \n",
    "        for line_num, line in enumerate(lines):\n",
    "            original_line = line\n",
    "            line = line.strip()\n",
    "            \n",
    "            if not line or line.startswith(';'):\n",
    "                continue\n",
    "            \n",
    "            # Check for basic block label (both named and numbered)\n",
    "            block_match = self.block_pattern.match(line) or self.numbered_block_pattern.match(line)\n",
    "            if block_match:\n",
    "                block_name = block_match.group(1)\n",
    "                current_block = cfg.add_block(block_name)\n",
    "                if not entry_found:\n",
    "                    cfg.entry_block = block_name\n",
    "                    entry_found = True\n",
    "                continue\n",
    "            \n",
    "            # Handle function entry (first instruction after define)\n",
    "            if not entry_found and current_block is None:\n",
    "                # First instruction creates entry block\n",
    "                current_block = cfg.add_block('entry')\n",
    "                cfg.entry_block = 'entry'\n",
    "                entry_found = True\n",
    "            \n",
    "            if current_block:\n",
    "                current_block.instructions.append(original_line.strip())\n",
    "                \n",
    "                # Enhanced terminator instruction detection\n",
    "                self._process_terminator_instruction(cfg, current_block, line)\n",
    "        \n",
    "        return cfg\n",
    "    \n",
    "    def _process_terminator_instruction(self, cfg: ControlFlowGraph, current_block: BasicBlock, line: str):\n",
    "        \"\"\"Process terminator instructions with enhanced pattern matching\"\"\"\n",
    "        # Conditional branch\n",
    "        branch_match = self.branch_pattern.search(line)\n",
    "        if branch_match:\n",
    "            true_target = branch_match.group(1)\n",
    "            false_target = branch_match.group(2)\n",
    "            cfg.add_edge(current_block.name, true_target, \"true\")\n",
    "            cfg.add_edge(current_block.name, false_target, \"false\")\n",
    "            return\n",
    "        \n",
    "        # Unconditional branch\n",
    "        unconditional_match = self.unconditional_branch_pattern.search(line)\n",
    "        if unconditional_match:\n",
    "            target = unconditional_match.group(1)\n",
    "            cfg.add_edge(current_block.name, target)\n",
    "            return\n",
    "        \n",
    "        # Switch statement\n",
    "        switch_match = self.switch_pattern.search(line)\n",
    "        if switch_match:\n",
    "            # Parse switch targets\n",
    "            targets_str = switch_match.group(1)\n",
    "            targets = re.findall(r'label\\s+%(\\w+)', targets_str)\n",
    "            for target in targets:\n",
    "                cfg.add_edge(current_block.name, target, \"switch\")\n",
    "            return\n",
    "        \n",
    "        # Indirect branch\n",
    "        indirectbr_match = self.indirectbr_pattern.search(line)\n",
    "        if indirectbr_match:\n",
    "            targets_str = indirectbr_match.group(1)\n",
    "            targets = re.findall(r'label\\s+%(\\w+)', targets_str)\n",
    "            for target in targets:\n",
    "                cfg.add_edge(current_block.name, target, \"indirect\")\n",
    "            return\n",
    "        \n",
    "        # Return instruction\n",
    "        if self.return_pattern.search(line):\n",
    "            cfg.exit_blocks.add(current_block.name)\n",
    "            return\n",
    "        \n",
    "        # Unreachable instruction\n",
    "        if 'unreachable' in line:\n",
    "            cfg.exit_blocks.add(current_block.name)\n",
    "            return"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e328290",
   "metadata": {},
   "source": [
    "### `CFGToGrammarConverter` *(from `llvm_cfg_generator.py`)*\n",
    "\n",
    "Synthesises a *context-free grammar* (CFG) from the raw control-flow graph\n",
    "such that **every** syntactically valid derivation corresponds to at least\n",
    "one concrete execution path inside the original function.\n",
    "\n",
    "The converter applies a set of *abstractions* (e.g. mapping every LLVM\n",
    "arithmetic instruction to the terminal symbol ``ADD``/``SUB``/â€¦) to keep\n",
    "the terminal alphabet tractable while preserving enough structural\n",
    "richness to guide greybox fuzzers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aec67b41",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CFGToGrammarConverter:\n",
    "    def __init__(self):\n",
    "        # Enhanced instruction abstraction for complex LLVM IR\n",
    "        self.instruction_abstraction = {\n",
    "            # Arithmetic operations\n",
    "            'add': 'ADD', 'fadd': 'FADD', 'sub': 'SUB', 'fsub': 'FSUB',\n",
    "            'mul': 'MUL', 'fmul': 'FMUL', 'udiv': 'UDIV', 'sdiv': 'SDIV',\n",
    "            'fdiv': 'FDIV', 'urem': 'UREM', 'srem': 'SREM', 'frem': 'FREM',\n",
    "            \n",
    "            # Bitwise operations\n",
    "            'shl': 'SHL', 'lshr': 'LSHR', 'ashr': 'ASHR', 'and': 'AND',\n",
    "            'or': 'OR', 'xor': 'XOR',\n",
    "            \n",
    "            # Memory operations\n",
    "            'alloca': 'ALLOCA', 'load': 'LOAD', 'store': 'STORE',\n",
    "            'getelementptr': 'GEP', 'fence': 'FENCE',\n",
    "            \n",
    "            # Conversion operations\n",
    "            'trunc': 'TRUNC', 'zext': 'ZEXT', 'sext': 'SEXT',\n",
    "            'fptrunc': 'FPTRUNC', 'fpext': 'FPEXT', 'fptoui': 'FPTOUI',\n",
    "            'fptosi': 'FPTOSI', 'uitofp': 'UITOFP', 'sitofp': 'SITOFP',\n",
    "            'ptrtoint': 'PTRTOINT', 'inttoptr': 'INTTOPTR', 'bitcast': 'BITCAST',\n",
    "            'addrspacecast': 'ADDRSPACECAST',\n",
    "            \n",
    "            # Other operations\n",
    "            'icmp': 'ICMP', 'fcmp': 'FCMP', 'phi': 'PHI', 'select': 'SELECT',\n",
    "            'call': 'CALL', 'va_arg': 'VA_ARG', 'landingpad': 'LANDINGPAD',\n",
    "            'cleanuppad': 'CLEANUPPAD', 'catchpad': 'CATCHPAD',\n",
    "            \n",
    "            # Terminator instructions\n",
    "            'ret': 'RETURN', 'br': 'BRANCH', 'switch': 'SWITCH',\n",
    "            'indirectbr': 'INDIRECT_BR', 'invoke': 'INVOKE', 'resume': 'RESUME',\n",
    "            'catchswitch': 'CATCHSWITCH', 'catchret': 'CATCHRET',\n",
    "            'cleanupret': 'CLEANUPRET', 'unreachable': 'UNREACHABLE',\n",
    "            \n",
    "            # Aggregate operations\n",
    "            'extractvalue': 'EXTRACTVALUE', 'insertvalue': 'INSERTVALUE',\n",
    "            'extractelement': 'EXTRACTELEMENT', 'insertelement': 'INSERTELEMENT',\n",
    "            'shufflevector': 'SHUFFLEVECTOR',\n",
    "            \n",
    "            # Atomic operations\n",
    "            'atomicrmw': 'ATOMICRMW', 'cmpxchg': 'CMPXCHG'\n",
    "        }\n",
    "        \n",
    "        # Enhanced patterns for complex control flow\n",
    "        self.control_flow_patterns = {\n",
    "            'loop': ['for', 'while', 'do_while'],\n",
    "            'conditional': ['if_then', 'if_then_else', 'ternary'],\n",
    "            'switch': ['multi_branch', 'jump_table'],\n",
    "            'exception': ['try_catch', 'cleanup', 'landing_pad']\n",
    "        }\n",
    "    \n",
    "    def convert_cfg_to_grammar(self, cfg: ControlFlowGraph, func_name: str) -> ContextFreeGrammar:\n",
    "        \"\"\"Convert a CFG to a context-free grammar optimized for effective fuzzing\"\"\"\n",
    "        grammar = ContextFreeGrammar()\n",
    "        grammar.start_symbol = f\"FUNC_{func_name.upper()}\"\n",
    "        \n",
    "        if not cfg.entry_block or not cfg.blocks:\n",
    "            return grammar\n",
    "        \n",
    "        # Generate comprehensive grammar rules that capture all control flow paths\n",
    "        self._generate_comprehensive_rules(grammar, cfg, func_name)\n",
    "        \n",
    "        return grammar\n",
    "    \n",
    "    def _generate_comprehensive_rules(self, grammar: ContextFreeGrammar, cfg: ControlFlowGraph, func_name: str):\n",
    "        \"\"\"Generate comprehensive grammar rules for effective fuzzing coverage\"\"\"\n",
    "        \n",
    "        # Main function entry point\n",
    "        func_symbol = f\"FUNC_{func_name.upper()}\"\n",
    "        grammar.add_rule(func_symbol, [f\"BLOCK_{cfg.entry_block.upper()}\"])\n",
    "        \n",
    "        # Generate rules for each basic block with all possible transitions\n",
    "        for block_name, block in cfg.blocks.items():\n",
    "            block_symbol = f\"BLOCK_{block_name.upper()}\"\n",
    "            \n",
    "            # Generate block content rules (instructions)\n",
    "            self._generate_block_content_rules(grammar, block, block_symbol)\n",
    "            \n",
    "            # Generate transition rules for control flow paths\n",
    "            self._generate_transition_rules(grammar, block, block_symbol)\n",
    "            \n",
    "            # Generate alternative path rules for fuzzing exploration\n",
    "            self._generate_alternative_path_rules(grammar, cfg, block, block_symbol)\n",
    "    \n",
    "    def _generate_block_content_rules(self, grammar: ContextFreeGrammar, block: BasicBlock, block_symbol: str):\n",
    "        \"\"\"Generate rules for the content of a basic block\"\"\"\n",
    "        if not block.instructions:\n",
    "            grammar.add_rule(block_symbol, [\"EPSILON\"])\n",
    "            return\n",
    "            \n",
    "        # Create instruction sequence rules\n",
    "        instruction_symbols = []\n",
    "        for i, instruction in enumerate(block.instructions):\n",
    "            inst_symbol = self._abstract_instruction(instruction)\n",
    "            if inst_symbol:\n",
    "                instruction_symbols.append(inst_symbol)\n",
    "        \n",
    "        if instruction_symbols:\n",
    "            # Single instruction rule\n",
    "            if len(instruction_symbols) == 1:\n",
    "                grammar.add_rule(block_symbol, instruction_symbols)\n",
    "            else:\n",
    "                # Multiple instruction sequence\n",
    "                grammar.add_rule(block_symbol, [\"INSTRUCTION_SEQ\"])\n",
    "                \n",
    "                # Create flexible instruction sequence rules for fuzzing\n",
    "                for i, inst in enumerate(instruction_symbols):\n",
    "                    if i == 0:\n",
    "                        grammar.add_rule(\"INSTRUCTION_SEQ\", [inst])\n",
    "                    grammar.add_rule(\"INSTRUCTION_SEQ\", [\"INSTRUCTION_SEQ\", inst])\n",
    "                \n",
    "                # Alternative single instruction rules for fuzzing variations\n",
    "                for inst in instruction_symbols:\n",
    "                    grammar.add_rule(\"INSTRUCTION_SEQ\", [inst])\n",
    "        else:\n",
    "            grammar.add_rule(block_symbol, [\"EPSILON\"])\n",
    "    \n",
    "    def _generate_transition_rules(self, grammar: ContextFreeGrammar, block: BasicBlock, block_symbol: str):\n",
    "        \"\"\"Generate control flow transition rules\"\"\"\n",
    "        \n",
    "        # No successors (termination)\n",
    "        if not block.successors:\n",
    "            grammar.add_rule(block_symbol, [block_symbol.replace(\"BLOCK_\", \"CONTENT_\")])\n",
    "            return\n",
    "        \n",
    "        # Single successor (sequential flow)\n",
    "        if len(block.successors) == 1:\n",
    "            successor_symbol = f\"BLOCK_{block.successors[0].upper()}\"\n",
    "            content_symbol = block_symbol.replace(\"BLOCK_\", \"CONTENT_\")\n",
    "            \n",
    "            # Sequential transition\n",
    "            grammar.add_rule(block_symbol, [content_symbol, successor_symbol])\n",
    "            grammar.add_rule(content_symbol, [\"INSTRUCTION_SEQ\"])\n",
    "            \n",
    "        # Multiple successors (conditional flow - choice points for fuzzing)\n",
    "        elif len(block.successors) == 2:\n",
    "            true_successor = f\"BLOCK_{block.successors[0].upper()}\"\n",
    "            false_successor = f\"BLOCK_{block.successors[1].upper()}\"\n",
    "            content_symbol = block_symbol.replace(\"BLOCK_\", \"CONTENT_\")\n",
    "            \n",
    "            # Conditional transition rules - critical for fuzzing path exploration\n",
    "            grammar.add_rule(block_symbol, [content_symbol, \"CHOICE_POINT\"])\n",
    "            grammar.add_rule(\"CHOICE_POINT\", [true_successor])\n",
    "            grammar.add_rule(\"CHOICE_POINT\", [false_successor])\n",
    "            grammar.add_rule(\"CHOICE_POINT\", [true_successor, false_successor])  # Fuzzing alternative\n",
    "            grammar.add_rule(content_symbol, [\"INSTRUCTION_SEQ\"])\n",
    "    \n",
    "    def _generate_alternative_path_rules(self, grammar: ContextFreeGrammar, cfg: ControlFlowGraph, \n",
    "                                       block: BasicBlock, block_symbol: str):\n",
    "        \"\"\"Generate alternative path rules to enhance fuzzing coverage\"\"\"\n",
    "        \n",
    "        # Create loop detection and alternative path rules\n",
    "        if self._is_loop_header(cfg, block.name):\n",
    "            loop_symbol = f\"LOOP_{block.name.upper()}\"\n",
    "            grammar.add_rule(loop_symbol, [block_symbol])\n",
    "            grammar.add_rule(loop_symbol, [block_symbol, loop_symbol])  # Loop iteration\n",
    "            grammar.add_rule(block_symbol, [loop_symbol])  # Alternative entry\n",
    "        \n",
    "        # Create convergence point rules for blocks with multiple predecessors\n",
    "        if len(block.predecessors) > 1:\n",
    "            merge_symbol = f\"MERGE_{block.name.upper()}\"\n",
    "            grammar.add_rule(merge_symbol, [block_symbol])\n",
    "            \n",
    "            # Alternative paths leading to this merge point\n",
    "            for pred in block.predecessors:\n",
    "                pred_symbol = f\"BLOCK_{pred.upper()}\"\n",
    "                grammar.add_rule(merge_symbol, [pred_symbol, block_symbol])\n",
    "        \n",
    "        # Create path variation rules for fuzzing\n",
    "        self._generate_path_variation_rules(grammar, block, block_symbol)\n",
    "    \n",
    "    def _generate_path_variation_rules(self, grammar: ContextFreeGrammar, block: BasicBlock, block_symbol: str):\n",
    "        \"\"\"Generate path variation rules specifically for fuzzing optimization\"\"\"\n",
    "        \n",
    "        # Create optional execution rules\n",
    "        optional_symbol = f\"OPT_{block.name.upper()}\"\n",
    "        grammar.add_rule(optional_symbol, [block_symbol])\n",
    "        grammar.add_rule(optional_symbol, [\"EPSILON\"])  # Optional execution for fuzzing\n",
    "        \n",
    "        # Create repetition rules for blocks that could be part of loops\n",
    "        if block.successors and block.name in block.successors:\n",
    "            repeat_symbol = f\"REPEAT_{block.name.upper()}\"\n",
    "            grammar.add_rule(repeat_symbol, [block_symbol])\n",
    "            grammar.add_rule(repeat_symbol, [block_symbol, repeat_symbol])\n",
    "            grammar.add_rule(block_symbol, [repeat_symbol])\n",
    "        \n",
    "        # Create interleaving rules for complex control flows\n",
    "        if len(block.successors) > 1:\n",
    "            interleave_symbol = f\"INTERLEAVE_{block.name.upper()}\"\n",
    "            for successor in block.successors:\n",
    "                succ_symbol = f\"BLOCK_{successor.upper()}\"\n",
    "                grammar.add_rule(interleave_symbol, [succ_symbol])\n",
    "                # Cross-product rules for fuzzing exploration\n",
    "                for other_successor in block.successors:\n",
    "                    if other_successor != successor:\n",
    "                        other_symbol = f\"BLOCK_{other_successor.upper()}\"\n",
    "                        grammar.add_rule(interleave_symbol, [succ_symbol, other_symbol])\n",
    "    \n",
    "    def _is_loop_header(self, cfg: ControlFlowGraph, block_name: str) -> bool:\n",
    "        \"\"\"Check if a block is a loop header by detecting back edges\"\"\"\n",
    "        if block_name not in cfg.blocks:\n",
    "            return False\n",
    "        \n",
    "        block = cfg.blocks[block_name]\n",
    "        \n",
    "        # Simple heuristic: check if any successor can reach back to this block\n",
    "        for successor in block.successors:\n",
    "            if self._can_reach(cfg, successor, block_name, visited=set()):\n",
    "                return True\n",
    "        \n",
    "        return False\n",
    "    \n",
    "    def _can_reach(self, cfg: ControlFlowGraph, from_block: str, to_block: str, visited: set) -> bool:\n",
    "        \"\"\"Check if from_block can reach to_block (for loop detection)\"\"\"\n",
    "        if from_block == to_block:\n",
    "            return True\n",
    "        \n",
    "        if from_block in visited or from_block not in cfg.blocks:\n",
    "            return False\n",
    "        \n",
    "        visited.add(from_block)\n",
    "        \n",
    "        for successor in cfg.blocks[from_block].successors:\n",
    "            if self._can_reach(cfg, successor, to_block, visited.copy()):\n",
    "                return True\n",
    "        \n",
    "        return False\n",
    "    \n",
    "    def _abstract_instruction(self, instruction: str) -> Optional[str]:\n",
    "        \"\"\"Abstract an LLVM instruction to a grammar terminal\"\"\"\n",
    "        instruction = instruction.strip()\n",
    "        if not instruction:\n",
    "            return None\n",
    "        \n",
    "        # Extract the operation from the instruction\n",
    "        for llvm_op, abstract_op in self.instruction_abstraction.items():\n",
    "            if llvm_op in instruction.lower():\n",
    "                return abstract_op\n",
    "        \n",
    "        # Default abstraction for unknown instructions\n",
    "        parts = instruction.split()\n",
    "        if parts:\n",
    "            first_word = parts[0].strip('=').strip('%')\n",
    "            if first_word and not first_word.isdigit():\n",
    "                return f\"OP_{first_word.upper()}\"\n",
    "        \n",
    "        return \"UNKNOWN_OP\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7568410c",
   "metadata": {},
   "source": [
    "### `llvm_ir_to_context_free_grammar` *(from `llvm_cfg_generator.py`)*\n",
    "\n",
    "One-shot convenience wrapper that runs the *entire* pipeline:\n",
    "\n",
    "1. Parse the textual IR into one :pyclass:`ControlFlowGraph` per function.\n",
    "2. Convert each CFG into a structural :pyclass:`ContextFreeGrammar`.\n",
    "\n",
    "The function is deliberately side-effect free â€“ no printing, no file I/O â€“\n",
    "to encourage re-use in batch processing and unit tests."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5e8a3b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def llvm_ir_to_context_free_grammar(llvm_ir_code: str) -> Dict[str, ContextFreeGrammar]:\n",
    "\n",
    "    parser = LLVMIRParser()\n",
    "    converter = CFGToGrammarConverter()\n",
    "    \n",
    "    # Step 1: Parse LLVM-IR and build CFGs\n",
    "    function_cfgs = parser.parse_llvm_ir(llvm_ir_code)\n",
    "    \n",
    "    # Step 2: Convert each CFG to a context-free grammar\n",
    "    function_grammars = {}\n",
    "    for func_name, cfg in function_cfgs.items():\n",
    "        grammar = converter.convert_cfg_to_grammar(cfg, func_name)\n",
    "        function_grammars[func_name] = grammar\n",
    "    \n",
    "    return function_grammars"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81669f8a",
   "metadata": {},
   "source": [
    "### `GrammarAnalytics` *(from `llvm_file_processor.py`)*\n",
    "\n",
    "Post-processing data structure that distills *quantitative* properties from\n",
    "a freshly generated :pyclass:`llvm_cfg_generator.ContextFreeGrammar`.\n",
    "\n",
    "The collected metrics fall into four broad categories:\n",
    "\n",
    "1. **Structural** â€“ number of rules, branching factor, derivation depth.\n",
    "2. **Control-flow** â€“ basic blocks, choice points, loop patterns.\n",
    "3. **Fuzzing-oriented** â€“ path alternatives, optional/repetition rules.\n",
    "4. **Coverage / Semantics** â€“ instruction types and high-level flow\n",
    "   patterns present in the grammar.\n",
    "\n",
    "A higher-level orchestrator (CLI, notebook, GUI) can serialise instances of\n",
    "this dataclass into human-readable reports or feed them into downstream\n",
    "optimisation heuristics that decide which functions are *worth* fuzzing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad6a4f61",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GrammarAnalytics:\n",
    "    \n",
    "    # Basic Grammar Metrics\n",
    "    function_name: str\n",
    "    total_rules: int\n",
    "    non_terminals_count: int\n",
    "    terminals_count: int\n",
    "    start_symbol: str\n",
    "    \n",
    "    # Control Flow Metrics\n",
    "    basic_blocks_count: int\n",
    "    choice_points_count: int\n",
    "    loop_patterns_count: int\n",
    "    instruction_sequences_count: int\n",
    "    \n",
    "    # Fuzzing Optimization Metrics\n",
    "    path_alternatives_count: int\n",
    "    optional_execution_rules: int\n",
    "    repetition_patterns: int\n",
    "    interleaving_rules: int\n",
    "    merge_points: int\n",
    "    \n",
    "    # Complexity Metrics\n",
    "    max_rule_length: int\n",
    "    avg_rule_length: float\n",
    "    branching_factor: float\n",
    "    depth_estimation: int\n",
    "    \n",
    "    # Coverage Metrics\n",
    "    instruction_types_covered: List[str]\n",
    "    control_flow_patterns: List[str]\n",
    "    fuzzing_readiness_score: float\n",
    "    \n",
    "    def __str__(self) -> str:\n",
    "        return f\"\"\"\n",
    "Grammar Analytics for {self.function_name}:\n",
    "========================================\n",
    "\n",
    "Basic Metrics:\n",
    "- Production Rules: {self.total_rules}\n",
    "- Non-terminals: {self.non_terminals_count}\n",
    "- Terminals: {self.terminals_count}\n",
    "- Start Symbol: {self.start_symbol}\n",
    "\n",
    "Control Flow Coverage:\n",
    "- Basic Blocks: {self.basic_blocks_count}\n",
    "- Choice Points: {self.choice_points_count}\n",
    "- Loop Patterns: {self.loop_patterns_count}\n",
    "- Instruction Sequences: {self.instruction_sequences_count}\n",
    "\n",
    "Fuzzing Optimization:\n",
    "- Path Alternatives: {self.path_alternatives_count}\n",
    "- Optional Executions: {self.optional_execution_rules}\n",
    "- Repetition Patterns: {self.repetition_patterns}\n",
    "- Interleaving Rules: {self.interleaving_rules}\n",
    "- Merge Points: {self.merge_points}\n",
    "\n",
    "Complexity Analysis:\n",
    "- Max Rule Length: {self.max_rule_length}\n",
    "- Avg Rule Length: {self.avg_rule_length:.2f}\n",
    "- Branching Factor: {self.branching_factor:.2f}\n",
    "- Estimated Depth: {self.depth_estimation}\n",
    "\n",
    "Coverage Details:\n",
    "- Instruction Types: {len(self.instruction_types_covered)} ({', '.join(self.instruction_types_covered[:5])}{'...' if len(self.instruction_types_covered) > 5 else ''})\n",
    "- Control Flow Patterns: {', '.join(self.control_flow_patterns)}\n",
    "- Fuzzing Readiness Score: {self.fuzzing_readiness_score:.2f}/10.0\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36d62cf2",
   "metadata": {},
   "source": [
    "### `FileProcessingResult` *(from `llvm_file_processor.py`)*\n",
    "\n",
    "Container returned by :pyfunc:`LLVMFileProcessor.process_file` and the\n",
    "helper wrappers such as :pyfunc:`process_llvm_file`.\n",
    "\n",
    "It captures both **high-level status** (success flag, aggregated metrics)\n",
    "and the **detailed artefacts** (per-function grammars + analytics).\n",
    "\n",
    "Having a single object makes it easier to forward data between batch\n",
    "processing helpers and report generators without having to juggle several\n",
    "loosely-coupled variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21e8a31f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FileProcessingResult:\n",
    "\n",
    "    \n",
    "    file_path: str\n",
    "    success: bool\n",
    "    error_message: Optional[str]\n",
    "    grammars: Dict[str, ContextFreeGrammar]\n",
    "    analytics: Dict[str, GrammarAnalytics]\n",
    "    \n",
    "    # File-level metrics\n",
    "    functions_processed: int\n",
    "    total_grammar_rules: int\n",
    "    total_choice_points: int\n",
    "    total_loop_patterns: int\n",
    "    overall_fuzzing_score: float\n",
    "    \n",
    "    def __str__(self) -> str:\n",
    "        if not self.success:\n",
    "            return f\"Failed to process {self.file_path}: {self.error_message}\"\n",
    "        \n",
    "        result = f\"\"\"\n",
    "Successfully processed: {self.file_path}\n",
    "=============================================\n",
    "\n",
    "File Summary:\n",
    "- Functions Processed: {self.functions_processed}\n",
    "- Total Grammar Rules: {self.total_grammar_rules}\n",
    "- Total Choice Points: {self.total_choice_points}\n",
    "- Total Loop Patterns: {self.total_loop_patterns}\n",
    "- Overall Fuzzing Score: {self.overall_fuzzing_score:.2f}/10.0\n",
    "\n",
    "Function Details:\n",
    "\"\"\"\n",
    "        \n",
    "        for func_name, analytics in self.analytics.items():\n",
    "            result += f\"\\nðŸ“Š {func_name}:\"\n",
    "            result += f\"\\n   Rules: {analytics.total_rules} | Choice Points: {analytics.choice_points_count} | Score: {analytics.fuzzing_readiness_score:.1f}/10\"\n",
    "        \n",
    "        return result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "823c37c5",
   "metadata": {},
   "source": [
    "### `LLVMFileProcessor` *(from `llvm_file_processor.py`)*\n",
    "\n",
    "Facade that orchestrates *reading*, *parsing*, *grammar generation*, and\n",
    "*analytics* for **one** LLVM-IR file at a time.\n",
    "\n",
    "The class deliberately hides all the gritty details so that command-line\n",
    "interfaces and GUIs can perform complex processing with a single method\n",
    "call."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5787277",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LLVMFileProcessor: \n",
    "    def __init__(self):\n",
    "        self.supported_extensions = ['.ll', '.bc', '.txt']\n",
    "    \n",
    "    def process_file(self, file_path: str) -> FileProcessingResult:\n",
    "        \n",
    "        # Validate file path\n",
    "        if not os.path.exists(file_path):\n",
    "            return FileProcessingResult(\n",
    "                file_path=file_path,\n",
    "                success=False,\n",
    "                error_message=f\"File not found: {file_path}\",\n",
    "                grammars={},\n",
    "                analytics={},\n",
    "                functions_processed=0,\n",
    "                total_grammar_rules=0,\n",
    "                total_choice_points=0,\n",
    "                total_loop_patterns=0,\n",
    "                overall_fuzzing_score=0.0\n",
    "            )\n",
    "        \n",
    "        # Check file extension\n",
    "        _, ext = os.path.splitext(file_path)\n",
    "        if ext not in self.supported_extensions:\n",
    "            return FileProcessingResult(\n",
    "                file_path=file_path,\n",
    "                success=False,\n",
    "                error_message=f\"Unsupported file extension: {ext}. Supported: {', '.join(self.supported_extensions)}\",\n",
    "                grammars={},\n",
    "                analytics={},\n",
    "                functions_processed=0,\n",
    "                total_grammar_rules=0,\n",
    "                total_choice_points=0,\n",
    "                total_loop_patterns=0,\n",
    "                overall_fuzzing_score=0.0\n",
    "            )\n",
    "        \n",
    "        try:\n",
    "            # Read LLVM-IR content\n",
    "            with open(file_path, 'r', encoding='utf-8') as f:\n",
    "                llvm_ir_content = f.read()\n",
    "            \n",
    "            # Generate grammars\n",
    "            grammars = llvm_ir_to_context_free_grammar(llvm_ir_content)\n",
    "            \n",
    "            if not grammars:\n",
    "                return FileProcessingResult(\n",
    "                    file_path=file_path,\n",
    "                    success=False,\n",
    "                    error_message=\"No functions found in LLVM-IR file\",\n",
    "                    grammars={},\n",
    "                    analytics={},\n",
    "                    functions_processed=0,\n",
    "                    total_grammar_rules=0,\n",
    "                    total_choice_points=0,\n",
    "                    total_loop_patterns=0,\n",
    "                    overall_fuzzing_score=0.0\n",
    "                )\n",
    "            \n",
    "            # Generate analytics for each grammar\n",
    "            analytics = {}\n",
    "            total_rules = 0\n",
    "            total_choice_points = 0\n",
    "            total_loop_patterns = 0\n",
    "            total_score = 0.0\n",
    "            \n",
    "            for func_name, grammar in grammars.items():\n",
    "                func_analytics = self._analyze_grammar(func_name, grammar)\n",
    "                analytics[func_name] = func_analytics\n",
    "                \n",
    "                total_rules += func_analytics.total_rules\n",
    "                total_choice_points += func_analytics.choice_points_count\n",
    "                total_loop_patterns += func_analytics.loop_patterns_count\n",
    "                total_score += func_analytics.fuzzing_readiness_score\n",
    "            \n",
    "            overall_score = total_score / len(grammars) if grammars else 0.0\n",
    "            \n",
    "            return FileProcessingResult(\n",
    "                file_path=file_path,\n",
    "                success=True,\n",
    "                error_message=None,\n",
    "                grammars=grammars,\n",
    "                analytics=analytics,\n",
    "                functions_processed=len(grammars),\n",
    "                total_grammar_rules=total_rules,\n",
    "                total_choice_points=total_choice_points,\n",
    "                total_loop_patterns=total_loop_patterns,\n",
    "                overall_fuzzing_score=overall_score\n",
    "            )\n",
    "            \n",
    "        except Exception as e:\n",
    "            return FileProcessingResult(\n",
    "                file_path=file_path,\n",
    "                success=False,\n",
    "                error_message=f\"Error processing file: {str(e)}\",\n",
    "                grammars={},\n",
    "                analytics={},\n",
    "                functions_processed=0,\n",
    "                total_grammar_rules=0,\n",
    "                total_choice_points=0,\n",
    "                total_loop_patterns=0,\n",
    "                overall_fuzzing_score=0.0\n",
    "            )\n",
    "    \n",
    "    def _analyze_grammar(self, func_name: str, grammar: ContextFreeGrammar) -> GrammarAnalytics:\n",
    "        \"\"\"Generate comprehensive analytics for a context-free grammar\"\"\"\n",
    "        \n",
    "        # Basic metrics\n",
    "        total_rules = len(grammar.rules)\n",
    "        non_terminals_count = len(grammar.non_terminals)\n",
    "        terminals_count = len(grammar.terminals)\n",
    "        \n",
    "        # Control flow metrics\n",
    "        basic_blocks = [nt for nt in grammar.non_terminals if nt.startswith('BLOCK_')]\n",
    "        choice_rules = [r for r in grammar.rules if 'CHOICE_POINT' in str(r)]\n",
    "        loop_rules = [r for r in grammar.rules if 'LOOP_' in str(r)]\n",
    "        instruction_rules = [r for r in grammar.rules if 'INSTRUCTION_SEQ' in str(r)]\n",
    "        \n",
    "        # Fuzzing optimization metrics\n",
    "        optional_rules = [r for r in grammar.rules if 'OPT_' in str(r)]\n",
    "        repetition_rules = [r for r in grammar.rules if 'REPEAT_' in str(r)]\n",
    "        interleaving_rules = [r for r in grammar.rules if 'INTERLEAVE_' in str(r)]\n",
    "        merge_rules = [r for r in grammar.rules if 'MERGE_' in str(r)]\n",
    "        \n",
    "        # Calculate path alternatives\n",
    "        path_alternatives = len(choice_rules) + len(optional_rules) + len(repetition_rules)\n",
    "        \n",
    "        # Complexity metrics\n",
    "        rule_lengths = [len(rule.rhs) for rule in grammar.rules]\n",
    "        max_rule_length = max(rule_lengths) if rule_lengths else 0\n",
    "        avg_rule_length = sum(rule_lengths) / len(rule_lengths) if rule_lengths else 0\n",
    "        \n",
    "        # Calculate branching factor\n",
    "        branching_factor = self._calculate_branching_factor(grammar)\n",
    "        \n",
    "        # Estimate grammar depth\n",
    "        depth_estimation = self._estimate_grammar_depth(grammar)\n",
    "        \n",
    "        # Identify instruction types and patterns\n",
    "        instruction_types = self._extract_instruction_types(grammar)\n",
    "        control_flow_patterns = self._identify_control_flow_patterns(grammar)\n",
    "        \n",
    "        # Calculate fuzzing readiness score\n",
    "        fuzzing_score = self._calculate_fuzzing_score(\n",
    "            choice_rules, loop_rules, optional_rules, \n",
    "            repetition_rules, interleaving_rules, basic_blocks\n",
    "        )\n",
    "        \n",
    "        return GrammarAnalytics(\n",
    "            function_name=func_name,\n",
    "            total_rules=total_rules,\n",
    "            non_terminals_count=non_terminals_count,\n",
    "            terminals_count=terminals_count,\n",
    "            start_symbol=grammar.start_symbol,\n",
    "            basic_blocks_count=len(basic_blocks),\n",
    "            choice_points_count=len(choice_rules),\n",
    "            loop_patterns_count=len(loop_rules),\n",
    "            instruction_sequences_count=len(instruction_rules),\n",
    "            path_alternatives_count=path_alternatives,\n",
    "            optional_execution_rules=len(optional_rules),\n",
    "            repetition_patterns=len(repetition_rules),\n",
    "            interleaving_rules=len(interleaving_rules),\n",
    "            merge_points=len(merge_rules),\n",
    "            max_rule_length=max_rule_length,\n",
    "            avg_rule_length=avg_rule_length,\n",
    "            branching_factor=branching_factor,\n",
    "            depth_estimation=depth_estimation,\n",
    "            instruction_types_covered=instruction_types,\n",
    "            control_flow_patterns=control_flow_patterns,\n",
    "            fuzzing_readiness_score=fuzzing_score\n",
    "        )\n",
    "    \n",
    "    def _calculate_branching_factor(self, grammar: ContextFreeGrammar) -> float:\n",
    "        \"\"\"Calculate the average branching factor of the grammar\"\"\"\n",
    "        nt_alternatives = {}\n",
    "        \n",
    "        for rule in grammar.rules:\n",
    "            if rule.lhs not in nt_alternatives:\n",
    "                nt_alternatives[rule.lhs] = 0\n",
    "            nt_alternatives[rule.lhs] += 1\n",
    "        \n",
    "        if not nt_alternatives:\n",
    "            return 0.0\n",
    "        \n",
    "        return sum(nt_alternatives.values()) / len(nt_alternatives)\n",
    "    \n",
    "    def _estimate_grammar_depth(self, grammar: ContextFreeGrammar) -> int:\n",
    "        \"\"\"Estimate the maximum derivation depth of the grammar\"\"\"\n",
    "        dependencies = {}\n",
    "        \n",
    "        for rule in grammar.rules:\n",
    "            dependencies[rule.lhs] = [symbol for symbol in rule.rhs if symbol in grammar.non_terminals]\n",
    "        \n",
    "        def max_depth(symbol, visited=None):\n",
    "            if visited is None:\n",
    "                visited = set()\n",
    "            if symbol in visited or symbol not in dependencies:\n",
    "                return 0\n",
    "            \n",
    "            visited.add(symbol)\n",
    "            max_child_depth = 0\n",
    "            \n",
    "            for child in dependencies[symbol]:\n",
    "                max_child_depth = max(max_child_depth, max_depth(child, visited.copy()))\n",
    "            \n",
    "            return 1 + max_child_depth\n",
    "        \n",
    "        return max_depth(grammar.start_symbol)\n",
    "    \n",
    "    def _extract_instruction_types(self, grammar: ContextFreeGrammar) -> List[str]:\n",
    "        \"\"\"Extract the types of instructions covered by the grammar\"\"\"\n",
    "        instruction_types = set()\n",
    "        \n",
    "        for terminal in grammar.terminals:\n",
    "            if terminal in ['ADD', 'SUB', 'MUL', 'DIV', 'LOAD', 'STORE', 'BRANCH', \n",
    "                          'ICMP', 'FCMP', 'ALLOCA', 'PHI', 'CALL', 'RETURN']:\n",
    "                instruction_types.add(terminal)\n",
    "        \n",
    "        return sorted(list(instruction_types))\n",
    "    \n",
    "    def _identify_control_flow_patterns(self, grammar: ContextFreeGrammar) -> List[str]:\n",
    "        \"\"\"Identify control flow patterns present in the grammar\"\"\"\n",
    "        patterns = []\n",
    "        \n",
    "        rule_strings = [str(rule) for rule in grammar.rules]\n",
    "        \n",
    "        if any('CHOICE_POINT' in rule_str for rule_str in rule_strings):\n",
    "            patterns.append('Conditional Branching')\n",
    "        \n",
    "        if any('LOOP_' in rule_str for rule_str in rule_strings):\n",
    "            patterns.append('Iterative Loops')\n",
    "        \n",
    "        if any('INSTRUCTION_SEQ' in rule_str for rule_str in rule_strings):\n",
    "            patterns.append('Sequential Execution')\n",
    "        \n",
    "        if any('MERGE_' in rule_str for rule_str in rule_strings):\n",
    "            patterns.append('Control Flow Convergence')\n",
    "        \n",
    "        if any('INTERLEAVE_' in rule_str for rule_str in rule_strings):\n",
    "            patterns.append('Complex Flow Interleaving')\n",
    "        \n",
    "        return patterns\n",
    "    \n",
    "    def _calculate_fuzzing_score(self, choice_rules, loop_rules, optional_rules, \n",
    "                                repetition_rules, interleaving_rules, basic_blocks) -> float:\n",
    "        \"\"\"\n",
    "        Calculate a fuzzing readiness score (0-10) based on grammar characteristics.\n",
    "        \n",
    "        Higher scores indicate better suitability for fuzzing applications.\n",
    "        \"\"\"\n",
    "        score = 0.0\n",
    "        \n",
    "        # Choice points are critical for fuzzing (0-3 points)\n",
    "        choice_score = min(3.0, len(choice_rules) * 0.5)\n",
    "        score += choice_score\n",
    "        \n",
    "        # Loop patterns enable iterative testing (0-2 points)\n",
    "        loop_score = min(2.0, len(loop_rules) * 0.3)\n",
    "        score += loop_score\n",
    "        \n",
    "        # Path alternatives provide fuzzing flexibility (0-2 points)\n",
    "        alternatives_score = min(2.0, (len(optional_rules) + len(repetition_rules)) * 0.2)\n",
    "        score += alternatives_score\n",
    "        \n",
    "        # Basic block coverage indicates completeness (0-2 points)\n",
    "        block_score = min(2.0, len(basic_blocks) * 0.2)\n",
    "        score += block_score\n",
    "        \n",
    "        # Complex flow patterns add fuzzing value (0-1 point)\n",
    "        complexity_score = min(1.0, len(interleaving_rules) * 0.1)\n",
    "        score += complexity_score\n",
    "        \n",
    "        return round(score, 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1af93770",
   "metadata": {},
   "source": [
    "### `process_llvm_file` *(from `llvm_file_processor.py`)*\n",
    "\n",
    "Thin wrapper around :pyclass:`LLVMFileProcessor` for one-off calls.\n",
    "\n",
    "This helper exists mostly for backwards compatibility and quick REPL\n",
    "experimentation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "affa6109",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_llvm_file(file_path: str) -> FileProcessingResult:\n",
    "    processor = LLVMFileProcessor()\n",
    "    return processor.process_file(file_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33dd3874",
   "metadata": {},
   "source": [
    "### `process_multiple_files` *(from `llvm_file_processor.py`)*\n",
    "\n",
    "Apply :pyfunc:`process_llvm_file` to every path in *file_paths*.\n",
    "\n",
    "The function collects the individual :pyclass:`FileProcessingResult`\n",
    "objects in a dictionary so that callers can inspect successes and failures\n",
    "side-by-side."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1725f40",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_multiple_files(file_paths: List[str]) -> Dict[str, FileProcessingResult]:\n",
    "    processor = LLVMFileProcessor()\n",
    "    results = {}\n",
    "    \n",
    "    for file_path in file_paths:\n",
    "        results[file_path] = processor.process_file(file_path)\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6614492e",
   "metadata": {},
   "source": [
    "### `generate_batch_report` *(from `llvm_file_processor.py`)*\n",
    "\n",
    "Human-readable summary for a whole *batch* of processed files.\n",
    "\n",
    "The returned string is already formatted for terminal output and therefore\n",
    "does **not** require additional pretty-printing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e0c321f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_batch_report(results: Dict[str, FileProcessingResult]) -> str:\n",
    "    successful_files = [r for r in results.values() if r.success]\n",
    "    failed_files = [r for r in results.values() if not r.success]\n",
    "    \n",
    "    if not results:\n",
    "        return \"No files processed.\"\n",
    "    \n",
    "    report = f\"\"\"\n",
    "Batch Processing Report\n",
    "======================\n",
    "\n",
    "Summary:\n",
    "- Total Files: {len(results)}\n",
    "- Successful: {len(successful_files)}\n",
    "- Failed: {len(failed_files)}\n",
    "\n",
    "\"\"\"\n",
    "    \n",
    "    if successful_files:\n",
    "        total_functions = sum(r.functions_processed for r in successful_files)\n",
    "        total_rules = sum(r.total_grammar_rules for r in successful_files)\n",
    "        total_choice_points = sum(r.total_choice_points for r in successful_files)\n",
    "        avg_score = sum(r.overall_fuzzing_score for r in successful_files) / len(successful_files)\n",
    "        \n",
    "        report += f\"\"\"\n",
    "Aggregate Statistics:\n",
    "- Functions Processed: {total_functions}\n",
    "- Grammar Rules Generated: {total_rules}\n",
    "- Total Choice Points: {total_choice_points}\n",
    "- Average Fuzzing Score: {avg_score:.2f}/10.0\n",
    "\n",
    "Successful Files:\n",
    "\"\"\"\n",
    "        for result in successful_files:\n",
    "            report += f\"  âœ… {result.file_path} ({result.functions_processed} functions, score: {result.overall_fuzzing_score:.1f})\\n\"\n",
    "    \n",
    "    if failed_files:\n",
    "        report += f\"\\nFailed Files:\\n\"\n",
    "        for result in failed_files:\n",
    "            report += f\"  âŒ {result.file_path}: {result.error_message}\\n\"\n",
    "    \n",
    "    return report"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47b79cd2",
   "metadata": {},
   "source": [
    "### `dump_grammars_to_file` *(from `llvm_file_processor.py`)*\n",
    "\n",
    "Serialise a set of grammars (plus optional analytics) into a text file.\n",
    "\n",
    "The resulting *dump* is aimed at researchers who prefer to inspect the\n",
    "grammar in a plain editor instead of loading the notebook / Python\n",
    "objects."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5b27478",
   "metadata": {},
   "outputs": [],
   "source": [
    "def dump_grammars_to_file(\n",
    "    grammars: Dict[str, ContextFreeGrammar],\n",
    "    output_path: str,\n",
    "    analytics: Dict[str, GrammarAnalytics] | None = None,\n",
    ") -> bool:\n",
    "    try:\n",
    "        with open(output_path, 'w', encoding='utf-8') as f:\n",
    "            f.write(\"=\"*80 + \"\\n\")\n",
    "            f.write(\"COMPREHENSIVE CONTEXT-FREE GRAMMAR DUMP\\n\")\n",
    "            f.write(\"=\"*80 + \"\\n\")\n",
    "            f.write(f\"Generated from LLVM-IR analysis\\n\")\n",
    "            f.write(f\"Total Functions: {len(grammars)}\\n\")\n",
    "            f.write(f\"Timestamp: {__import__('datetime').datetime.now()}\\n\")\n",
    "            f.write(\"=\"*80 + \"\\n\\n\")\n",
    "            \n",
    "            # Summary statistics\n",
    "            total_rules = sum(len(g.rules) for g in grammars.values())\n",
    "            total_non_terminals = sum(len(g.non_terminals) for g in grammars.values())\n",
    "            total_terminals = sum(len(g.terminals) for g in grammars.values())\n",
    "            \n",
    "            f.write(\"SUMMARY STATISTICS\\n\")\n",
    "            f.write(\"-\"*50 + \"\\n\")\n",
    "            f.write(f\"Total Production Rules: {total_rules}\\n\")\n",
    "            f.write(f\"Total Non-terminals: {total_non_terminals}\\n\")\n",
    "            f.write(f\"Total Terminals: {total_terminals}\\n\")\n",
    "            f.write(f\"Average Rules per Function: {total_rules/len(grammars):.2f}\\n\\n\")\n",
    "            \n",
    "            # If analytics available, show fuzzing readiness overview\n",
    "            if analytics:\n",
    "                avg_fuzzing_score = sum(a.fuzzing_readiness_score for a in analytics.values()) / len(analytics)\n",
    "                total_choice_points = sum(a.choice_points_count for a in analytics.values())\n",
    "                total_loops = sum(a.loop_patterns_count for a in analytics.values())\n",
    "                \n",
    "                f.write(\"FUZZING READINESS OVERVIEW\\n\")\n",
    "                f.write(\"-\"*50 + \"\\n\")\n",
    "                f.write(f\"Average Fuzzing Score: {avg_fuzzing_score:.2f}/10.0\\n\")\n",
    "                f.write(f\"Total Choice Points: {total_choice_points}\\n\")\n",
    "                f.write(f\"Total Loop Patterns: {total_loops}\\n\")\n",
    "                f.write(f\"Functions with High Fuzzing Score (>7.0): {sum(1 for a in analytics.values() if a.fuzzing_readiness_score > 7.0)}\\n\\n\")\n",
    "            \n",
    "            # Detailed grammar dump for each function\n",
    "            for i, (func_name, grammar) in enumerate(grammars.items(), 1):\n",
    "                f.write(\"=\"*80 + \"\\n\")\n",
    "                f.write(f\"FUNCTION {i}: {func_name}\\n\")\n",
    "                f.write(\"=\"*80 + \"\\n\\n\")\n",
    "                \n",
    "                # Basic grammar information\n",
    "                f.write(\"GRAMMAR OVERVIEW\\n\")\n",
    "                f.write(\"-\"*40 + \"\\n\")\n",
    "                f.write(f\"Start Symbol: {grammar.start_symbol}\\n\")\n",
    "                f.write(f\"Production Rules: {len(grammar.rules)}\\n\")\n",
    "                f.write(f\"Non-terminals: {len(grammar.non_terminals)}\\n\")\n",
    "                f.write(f\"Terminals: {len(grammar.terminals)}\\n\\n\")\n",
    "                \n",
    "                # Analytics if available\n",
    "                if analytics and func_name in analytics:\n",
    "                    anal = analytics[func_name]\n",
    "                    f.write(\"FUZZING ANALYTICS\\n\")\n",
    "                    f.write(\"-\"*40 + \"\\n\")\n",
    "                    f.write(f\"Fuzzing Readiness Score: {anal.fuzzing_readiness_score:.2f}/10.0\\n\")\n",
    "                    f.write(f\"Basic Blocks: {anal.basic_blocks_count}\\n\")\n",
    "                    f.write(f\"Choice Points: {anal.choice_points_count}\\n\")\n",
    "                    f.write(f\"Loop Patterns: {anal.loop_patterns_count}\\n\")\n",
    "                    f.write(f\"Path Alternatives: {anal.path_alternatives_count}\\n\")\n",
    "                    f.write(f\"Branching Factor: {anal.branching_factor:.2f}\\n\")\n",
    "                    f.write(f\"Estimated Depth: {anal.depth_estimation}\\n\")\n",
    "                    f.write(f\"Instruction Types: {', '.join(anal.instruction_types_covered[:10])}\\n\")\n",
    "                    if len(anal.instruction_types_covered) > 10:\n",
    "                        f.write(f\"... and {len(anal.instruction_types_covered) - 10} more\\n\")\n",
    "                    f.write(f\"Control Flow Patterns: {', '.join(anal.control_flow_patterns)}\\n\\n\")\n",
    "                \n",
    "                # Non-terminals\n",
    "                f.write(\"NON-TERMINALS\\n\")\n",
    "                f.write(\"-\"*40 + \"\\n\")\n",
    "                sorted_nts = sorted(grammar.non_terminals)\n",
    "                for j in range(0, len(sorted_nts), 8):\n",
    "                    f.write(\"  \" + \", \".join(sorted_nts[j:j+8]) + \"\\n\")\n",
    "                f.write(\"\\n\")\n",
    "                \n",
    "                # Terminals\n",
    "                f.write(\"TERMINALS\\n\")\n",
    "                f.write(\"-\"*40 + \"\\n\")\n",
    "                sorted_ts = sorted(grammar.terminals)\n",
    "                for j in range(0, len(sorted_ts), 10):\n",
    "                    f.write(\"  \" + \", \".join(sorted_ts[j:j+10]) + \"\\n\")\n",
    "                f.write(\"\\n\")\n",
    "                \n",
    "                # Production rules organized by categories\n",
    "                f.write(\"PRODUCTION RULES\\n\")\n",
    "                f.write(\"-\"*40 + \"\\n\")\n",
    "                \n",
    "                # Group rules by left-hand side\n",
    "                rules_by_lhs = {}\n",
    "                for rule in grammar.rules:\n",
    "                    if rule.lhs not in rules_by_lhs:\n",
    "                        rules_by_lhs[rule.lhs] = []\n",
    "                    rules_by_lhs[rule.lhs].append(rule.rhs)\n",
    "                \n",
    "                # Show rules in organized categories\n",
    "                categories = {\n",
    "                    'Function Entry': ['FUNC_', 'ENTRY_'],\n",
    "                    'Basic Blocks': ['BLOCK_', 'BB_'],\n",
    "                    'Control Flow': ['CHOICE_POINT', 'BRANCH_', 'CONDITIONAL_'],\n",
    "                    'Loops': ['LOOP_', 'WHILE_', 'FOR_', 'REPEAT_'],\n",
    "                    'Instructions': ['INSTRUCTION_SEQ', 'INST_', 'OP_'],\n",
    "                    'Data Flow': ['PHI_', 'SELECT_', 'ASSIGN_'],\n",
    "                    'Memory Operations': ['LOAD_', 'STORE_', 'ALLOCA_'],\n",
    "                    'Paths & Alternatives': ['PATH_', 'ALT_', 'OPT_', 'CHOICE_'],\n",
    "                    'Other': []\n",
    "                }\n",
    "                \n",
    "                for category, patterns in categories.items():\n",
    "                    category_rules = []\n",
    "                    for lhs in sorted(rules_by_lhs.keys()):\n",
    "                        if patterns and any(pattern in lhs for pattern in patterns):\n",
    "                            category_rules.append(lhs)\n",
    "                        elif not patterns:  # 'Other' category\n",
    "                            if not any(any(p in lhs for p in pats) for pats in list(categories.values())[:-1]):\n",
    "                                category_rules.append(lhs)\n",
    "                    \n",
    "                    if category_rules:\n",
    "                        f.write(f\"\\n{category} Rules:\\n\")\n",
    "                        for lhs in category_rules[:15]:  # Limit to first 15 rules per category\n",
    "                            alternatives = rules_by_lhs[lhs]\n",
    "                            f.write(f\"{lhs} ->\")\n",
    "                            for k, rhs in enumerate(alternatives[:3]):  # Show up to 3 alternatives\n",
    "                                connector = \" |\" if k > 0 else \"\"\n",
    "                                rhs_str = ' '.join(rhs) if rhs else 'EPSILON'\n",
    "                                if len(rhs_str) > 80:\n",
    "                                    rhs_str = rhs_str[:77] + \"...\"\n",
    "                                f.write(f\"{connector} {rhs_str}\\n\")\n",
    "                                if k == 0:\n",
    "                                    f.write(\"     \")\n",
    "                            if len(alternatives) > 3:\n",
    "                                f.write(f\"      | ... and {len(alternatives) - 3} more alternatives\\n\")\n",
    "                        \n",
    "                        if len(category_rules) > 15:\n",
    "                            f.write(f\"... and {len(category_rules) - 15} more {category.lower()} rules\\n\")\n",
    "                \n",
    "                f.write(\"\\n\" + \"-\"*80 + \"\\n\\n\")\n",
    "            \n",
    "            f.write(\"=\"*80 + \"\\n\")\n",
    "            f.write(\"END OF GRAMMAR DUMP\\n\")\n",
    "            f.write(\"=\"*80 + \"\\n\")\n",
    "        \n",
    "        return True\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error writing grammar dump to {output_path}: {e}\")\n",
    "        return False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "691b2638",
   "metadata": {},
   "source": [
    "### `dump_grammars_from_file` *(from `llvm_file_processor.py`)*\n",
    "\n",
    "Utility that combines :pyfunc:`process_llvm_file` **and**\n",
    ":pyfunc:`dump_grammars_to_file` for one stop *CLI* convenience."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9d6e190",
   "metadata": {},
   "outputs": [],
   "source": [
    "def dump_grammars_from_file(llvm_file_path: str, output_path: str | None = None) -> bool:\n",
    "    # Auto-generate output path if not provided\n",
    "    if output_path is None:\n",
    "        base_name = os.path.splitext(os.path.basename(llvm_file_path))[0]\n",
    "        output_path = f\"{base_name}_grammar_dump.txt\"\n",
    "    \n",
    "    # Process the LLVM file\n",
    "    result = process_llvm_file(llvm_file_path)\n",
    "    \n",
    "    if not result.success:\n",
    "        print(f\"âŒ Failed to process {llvm_file_path}: {result.error_message}\")\n",
    "        return False\n",
    "    \n",
    "    if not result.grammars:\n",
    "        print(f\"âŒ No grammars generated from {llvm_file_path}\")\n",
    "        return False\n",
    "    \n",
    "    # Dump grammars to file\n",
    "    success = dump_grammars_to_file(result.grammars, output_path, result.analytics)\n",
    "    \n",
    "    if success:\n",
    "        print(f\"Grammar dump saved to: {output_path}\")\n",
    "        print(f\"Processed {result.functions_processed} functions\")\n",
    "        print(f\"Generated {result.total_grammar_rules} grammar rules\")\n",
    "        print(f\"Overall fuzzing score: {result.overall_fuzzing_score:.1f}/10.0\")\n",
    "    else:\n",
    "        print(f\"Failed to save grammar dump to {output_path}\")\n",
    "    \n",
    "    return success"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ab4de8d",
   "metadata": {},
   "source": [
    "### `extract_grammar_from_file` *(from `llvm_grammar_extractor.py`)*\n",
    "\n",
    "High-level helper that loads *llvm_file_path* and converts it into\n",
    "grammars using the core pipeline from :pymod:`llvm_cfg_generator`.\n",
    "\n",
    "This is the *lowest* layer in the extractor module; every other utility\n",
    "eventually calls it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6de3ff9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_grammar_from_file(llvm_file_path: str) -> Dict[str, ContextFreeGrammar]:\n",
    "    \n",
    "    # Validate file exists\n",
    "    if not os.path.exists(llvm_file_path):\n",
    "        raise FileNotFoundError(f\"LLVM-IR file not found: {llvm_file_path}\")\n",
    "    \n",
    "    try:\n",
    "        # Read file content\n",
    "        with open(llvm_file_path, 'r', encoding='utf-8') as f:\n",
    "            llvm_ir_content = f.read()\n",
    "        \n",
    "        # Generate grammars\n",
    "        grammars = llvm_ir_to_context_free_grammar(llvm_ir_content)\n",
    "        \n",
    "        if not grammars:\n",
    "            raise ValueError(\"No functions found in LLVM-IR file\")\n",
    "        \n",
    "        return grammars\n",
    "        \n",
    "    except IOError as e:\n",
    "        raise IOError(f\"Error reading file {llvm_file_path}: {e}\")\n",
    "    except Exception as e:\n",
    "        raise ValueError(f\"Error processing LLVM-IR: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e14e3efd",
   "metadata": {},
   "source": [
    "### `extract_single_function_grammar` *(from `llvm_grammar_extractor.py`)*\n",
    "\n",
    "Thin convenience wrapper around :pyfunc:`extract_grammar_from_file`.\n",
    "\n",
    "It simply returns ``grammars.get(function_name)`` so that callers do not\n",
    "have to perform the dictionary look-up in two lines of code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bd7a4de",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_single_function_grammar(llvm_file_path: str, function_name: str) -> Optional[ContextFreeGrammar]:\n",
    "    \n",
    "    grammars = extract_grammar_from_file(llvm_file_path)\n",
    "    return grammars.get(function_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a75e5ec",
   "metadata": {},
   "source": [
    "### `extract_grammar_from_string` *(from `llvm_grammar_extractor.py`)*\n",
    "\n",
    "Like :pyfunc:`extract_grammar_from_file` but works on an in-memory\n",
    "string instead of reading from disk."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ea8f91c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_grammar_from_string(llvm_ir_code: str) -> Dict[str, ContextFreeGrammar]:\n",
    "    \n",
    "    if not llvm_ir_code or not llvm_ir_code.strip():\n",
    "        raise ValueError(\"Empty LLVM-IR code provided\")\n",
    "    \n",
    "    try:\n",
    "        grammars = llvm_ir_to_context_free_grammar(llvm_ir_code)\n",
    "        \n",
    "        if not grammars:\n",
    "            raise ValueError(\"No functions found in LLVM-IR code\")\n",
    "        \n",
    "        return grammars\n",
    "        \n",
    "    except Exception as e:\n",
    "        raise ValueError(f\"Error processing LLVM-IR: {e}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "pygments_lexer": "ipython3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
